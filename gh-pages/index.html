<!DOCTYPE html>
<html>
<!--------------------------------------------------
    "The aim of science is to seek the simplest 
    explanations of complex facts. We are apt to 
    fall into the error of thinking that the facts 
    are simple because simplicity is the goal of 
    our quest. The guiding motto in the life of 
    every natural philosopher should be:
         
         'Seek simplicity and distrust it.'"
                
                -Alfred North Whitehead
---------------------------------------------------->

<head>
    <title>Effective Information in DNNs</title>
    
    <!--------------------------------------------------
                Import and Define CSS Things!
    ---------------------------------------------------->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="/assets/homepage.css">
    
    <!--------------------------------------------------
                        Site Icon
    ---------------------------------------------------->
    <link rel="apple-touch-icon" sizes="180x180" href="/assets/favicon/apple-touch-icon.png">
    <link rel="icon" type="image/png" sizes="32x32" href="/assets/favicon/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/assets/favicon/favicon-16x16.png">
    <link rel="manifest" href="/assets/favicon/site.webmanifest">
    <link rel="mask-icon" href="/assets/favicon/safari-pinned-tab.svg" color="#5bbad5">
    <link rel="shortcut icon" href="/assets/favicon/favicon.ico">
    <meta name="msapplication-TileColor" content="#da532c">
    <meta name="msapplication-config" content="/assets/favicon/browserconfig.xml">
    <meta name="theme-color" content="#ffffff">
    
    <!--------------------------------------------------
        KaTeX - Render LaTeX Expressions in Document
        https://github.com/Khan/KaTeX/blob/master/contrib/auto-render/README.md
    ---------------------------------------------------->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css" integrity="sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei" crossorigin="anonymous">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js" integrity="sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js" integrity="sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body,
            {
              delimiters: [
                  {left: "$$", right: "$$", display: true},
                  {left: "$", right: "$", display: false},
              ]
          });
    });
    </script>

</head>

<body>

<!------------------------
         Header 
(insert at the top of body)
-------------------------->
<!-- <div class="navbar">
	<ul>
		<a href="/"><li>Eric J. Michaud</li></a>
		<a href="/blog/"><li>Blog</li></a>
		<a href="/contact/"><li>Contact</li></a>
	</ul> 
</div>
<div id="bar"></div> -->

<!------------------------
      Webpage Content
(insert in middle of body)
-------------------------->
<div class="content">
    <!-- <div class="container"> -->
    <h1>Examining the causal structures of artificial neural networks using information theory: Videos</h1>
    <p>This post accompanies the paper <a href="https://insert_arxiv_link">Examining the causal structures of artificial neural networks</a> by Simon Mattsson, <a href="https://ericjmichaud.com">Eric J. Michaud</a>, and <a href="https://www.erikphoel.com/">Erik Hoel</a>. In the paper, we define the cumulative effective information ($EI$) between a layer $L_1$ and $L_2$ to be:

    $$ EI(L_1 \rightarrow L_2) = \sum_{(A \in L_1,B \in L_2)} I(t_A,t_B) \ | \ do(L_1=H^{\max}) $$

    Each term in this sum is a mutual information between the activation of neuron $A \in L_1$ and neuron $B \in L_2$ when $A$ and all the other neurons in $L_1$ are firing randomly. This measures the extent to which the activation of $A$ can influence the activation of $B$ despite the presence of maximum-entropy noise from all the other neurons which B's activation depends on. We can also define the cumulative sensitivity of the connections:
    
    $$ Sensitivity(L_1 \rightarrow L_2) = \sum_{(A \in L_1, B \in L_2)} I(t_A, t_B) \ | \ do(A=H^{\max})$$

    Sensitivity measures the extent to which the activation of $A$ can influence the activation of $B$, without the presence of any noise from other neurons in $L_1$. When $sensitivity$ and $EI$ are measured with sufficiently small bins, $EI \leq sensitivity$ (the mutual information between $A \in L_1$ and $B \in L_2$ decreases when other neurons in $L_1$ are firing). We call the difference between $sensitivity$ and $EI$ the $degeneracy$ of the layer:

    $$ EI = sensitivity - degeneracy$$

    For each layer $L_1 \rightarrow L_2$ in a neural network, we can measure sensitivity and degeneracy, and visualize how they increase or decrease throughout training in the "causal plane". When a layer moves vertically in the plane, then its $sensitivity$ and $degeneracy$ are both increasing while its $EI$ remains constant. Horizontal movement represents changes in $sensitivity$ or $degeneracy$ which do change the $EI$ of the layer.</p>

    <p>Below, we show videos of how the layers of a neural network move and differentiate in the causal plane during training. For networks trained on MNIST, we observe that layers differentiate in the causal plane, and we conjecture that this differentiation reflects that the layers are assuming distinct roles in the structure of the network.</p>

    <p>The videos below show movement in the causal plane for a variety of network topologies:</p>

    <h2>100->30->10 on MNIST</h2>
    <video width="100%" controls>
    <source src="https://ericjmichaud.com/downloads/ei-videos/mnist_100-30-10_plane_video.mp4" type="video/mp4">
    Your browser does not support the video tag.
    </video>

    <h2>100 -> 30->30->30->10 on MNIST</h2>
    <video width="100%" controls>
    <source src="https://ericjmichaud.com/downloads/ei-videos/mnist_100-30-30-30-10_plane_video.mp4" type="video/mp4">
    Your browser does not support the video tag.
    </video>

    <h2>100->30->30->30->30->30->10 on MNIST</h2>
    <video width="100%" controls>
    <source src="https://ericjmichaud.com/downloads/ei-videos/mnist_100-30-30-30-30-30-10_plane_video.mp4" type="video/mp4">
    Your browser does not support the video tag.
    </video>

    <p>Generally, the hidden layers move mostly vertically (keeping $EI$ approximately constant) during learning, while the first and last layers become more degenerate. In the hidden layers, we see a drift towards degeneracy right as the network begins to overfit. In our deepest network (100->30->30->30->30->30->10), we observe that the hidden layers buldge out from the $EI$ nullcline, with the earlier layers becoming more sensitive and the later hidden layers becoming more degenerate, however the hidden layers converge to the same region of the plane as the network begins to overfit, before drifting towards degeneracy. 

    We see:
    <ol>
        <li>the most movement in the plane during periods when the loss is being reduced the fastest</li>
        <li>transitions in behavior as the network begins to overfit</li>
        <li>differentiation between the behavior of the first layer, the hidden layers, and the last layer</li>
    </ol>

    Furthermore, the behavior does not appear to merely reflect the shape of the layer. Conider the following network:</p>

    <h2>100->30->20->30->10->30->10 on MNIST</h2>
    <video width="100%" controls>
    <source src="https://ericjmichaud.com/downloads/ei-videos/mnist_100-30-20-30-10-30-10_plane_video.mp4" type="video/mp4">
    Your browser does not support the video tag.
    </video>

    <p>When we mix up the widths of the hidden layers, we see similar behavior as before. Despite layers 4 and 6 having the same shape (30->10), their behavior is very different. Layer 4 (a hidden layer) moves little (slightly vertically then slightly towards degeneracy during overfitting), whereas layer 6 moves significantly towards degeneracy early in training. Layers 2 and 3, despite having opposite shapes (30->20 vs 20->30), and starting in different places, move in about the same way. All hidden layers switch directions and to drift towards degeneracy right as overfitting starts. </p>

    <p>It therefore seems that the behavior in the causal plane reflects something nontrivial about the each layer's role in the network, and about how the structure of the network changes as it switches from generalization to overfitting the data.</p> 

</div>

<!------------------------
         Footer 
(insert at bottom of body)
-------------------------->
<!-- <div class="footer">
    <p>Eric J. Michaud - 2020</p>
</div> -->
    
</body>
</html>
