<!DOCTYPE html>
<html>
<!--------------------------------------------------
    "The aim of science is to seek the simplest 
    explanations of complex facts. We are apt to 
    fall into the error of thinking that the facts 
    are simple because simplicity is the goal of 
    our quest. The guiding motto in the life of 
    every natural philosopher should be:
         
         'Seek simplicity and distrust it.'"
                
                -Alfred North Whitehead
---------------------------------------------------->

<head>
    <title>Effective Information in DNNs</title>
    
    <!--------------------------------------------------
                Import and Define CSS Things!
    ---------------------------------------------------->
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="stylesheet" href="assets/homepage.css">
    
    
    <!--------------------------------------------------
        KaTeX - Render LaTeX Expressions in Document
        https://github.com/Khan/KaTeX/blob/master/contrib/auto-render/README.md
    ---------------------------------------------------->
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.css" integrity="sha384-TEMocfGvRuD1rIAacqrknm5BQZ7W7uWitoih+jMNFXQIbNl16bO8OZmylH/Vi/Ei" crossorigin="anonymous">
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/katex.min.js" integrity="sha384-jmxIlussZWB7qCuB+PgKG1uLjjxbVVIayPJwi6cG6Zb4YKq0JIw+OMnkkEC7kYCq" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.9.0/contrib/auto-render.min.js" integrity="sha384-IiI65aU9ZYub2MY9zhtKd1H2ps7xxf+eb2YFG9lX6uRqpXCvBTOidPRCXCrQ++Uc" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
      renderMathInElement(document.body,
            {
              delimiters: [
                  {left: "$$", right: "$$", display: true},
                  {left: "$", right: "$", display: false},
              ]
          });
    });
    </script>

</head>

<body>

<!------------------------
         Header 
(insert at the top of body)
-------------------------->
<!-- <div class="navbar">
	<ul>
		<a href="/"><li>Eric J. Michaud</li></a>
		<a href="/blog/"><li>Blog</li></a>
		<a href="/contact/"><li>Contact</li></a>
	</ul> 
</div>
<div id="bar"></div> -->

<!------------------------
      Webpage Content
(insert in middle of body)
-------------------------->
<div class="content">
    <!-- <div class="container"> -->
    <h1>Examining the causal structures of artificial neural networks using information theory: Videos</h1>
    <p>This post accompanies the paper <a href="https://arxiv.org/abs/2010.13871">Examining the causal structures of artificial neural networks</a> by Simon Mattsson, <a href="https://ericjmichaud.com">Eric J. Michaud</a>, and <a href="https://www.erikphoel.com/">Erik Hoel</a>. In the paper, we define the Effective Information ($EI$) between a layer $L_1$ and $L_2$ to be:

    $$ EI(L_1 \rightarrow L_2) = I(L_1, L_2) \ | \ do(L_1 = H^{\max}). $$

    This is a mutual information between the vector of activations output by neurons in $L_1$ and the vector activations in $L_2$, where the neurons in $L_1$ take values independently and uniformly in their activation output range. We can also define the $sensitivity$:
    
    $$ Sensitivity(L_1 \rightarrow L_2) = \sum_{(A \in L_1, B \in L_2)} I(t_A, t_B) \ | \ do(A=H^{\max})$$

    Sensitivity sums over mutual information between individual neurons in $A \in L_1$ and $B \in L_2$, where only $A$ fires (taking values uniformly in its output range), and with all other neurons in $L_1$ outputting 0. When $sensitivity$ and $EI$ are measured with sufficiently small bins, $EI \leq sensitivity$. We call the difference between $sensitivity$ and $EI$ the $degeneracy$ of the layer:

    $$ EI = sensitivity - degeneracy$$

    For each layer $L_1 \rightarrow L_2$ in a neural network, we can measure sensitivity and degeneracy, and visualize how they increase or decrease throughout training in the "causal plane". When a layer moves vertically in the plane, then its $sensitivity$ and $degeneracy$ are both increasing while its $EI$ remains constant. Horizontal movement represents changes in $sensitivity$ or $degeneracy$ which do change the $EI$ of the layer.</p>

    <p>Below, we show videos of how the layers of a neural network move and differentiate in the causal plane during training. For networks trained on MNIST, we observe that layers differentiate in the causal plane, and we conjecture that this differentiation reflects that the layers are assuming distinct roles in the structure of the network.</p>

    <p>The videos below show movement in the causal plane as networks are trained on MNIST (with inputs reduced from 28x28 to 5x5, and with only digits 0-4 in the dataset) and Iris.</p>

    <h2>25->6->6->6->5 on MNIST</h2>
    <video width="100%" controls>
    <source src="assets/videos/mnist_25-6-6-6-5_plane_video.mp4" type="video/mp4">
    Your browser does not support the video tag.
    </video>

    <h2>4->5->5->3 on Iris</h2>
    <video width="100%" controls>
    <source src="assets/videos/iris_4-5-5-3_plane_video.mp4" type="video/mp4">
    Your browser does not support the video tag.
    </video>

    <p>Some observations:
        <ol>
            <li>In the hidden layers, we see a drift towards degeneracy right as the network begins to overfit.</li>
            <li>The most movement in the plane occurs during periods when the loss is being reduced the fastest.</li>
            <li>We see transitions in behavior as the network begins to overfit</li>
            <li>We see differentiation between the behavior of the first layer, the hidden layers, and the last layer on MNIST</li>
        </ol>
    </p>

</div>

<!------------------------
         Footer 
(insert at bottom of body)
-------------------------->
<!-- <div class="footer">
    <p>Eric J. Michaud - 2020</p>
</div> -->
    
</body>
</html>
